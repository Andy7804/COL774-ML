{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image_file(image_path, target=(100, 100)):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image at path {image_path} could not be loaded.\")\n",
    "    \n",
    "    # Get original dimensions\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Center crop (crop to the smallest dimension to make it square)\n",
    "    min_dim = min(h, w)\n",
    "    start_x = (w - min_dim) // 2\n",
    "    start_y = (h - min_dim) // 2\n",
    "    img = img[start_y:start_y + min_dim, start_x:start_x + min_dim]\n",
    "    \n",
    "    # Resize to 100x100\n",
    "    img = cv2.resize(img, target)\n",
    "    \n",
    "    # # Convert grayscale to RGB if needed\n",
    "    # if len(img.shape) == 2:  # Grayscale image\n",
    "    #     img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # # Convert RGBA to RGB if needed\n",
    "    # if img.shape[2] == 4:  # RGBA image\n",
    "    #     img = img[:, :, :3]\n",
    "    \n",
    "    # Normalize pixel values to [0,1]\n",
    "    img = img / 255.0\n",
    "\n",
    "    # Flatten to a 1D vector of length 30,000\n",
    "    img = img.flatten()\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_image(image_dir, class_label):\n",
    "    # returns X and Y numpy arrays of images and labels\n",
    "    # skip = 0\n",
    "    X = []\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for valid image files\n",
    "            image_path = os.path.join(image_dir, filename)\n",
    "            # Resize and center crop the image\n",
    "            try:\n",
    "                image = preprocess_image_file(image_path)\n",
    "                X.append(image)\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "\n",
    "    # Create a NumPy array from the list of images\n",
    "    X = np.array(X)\n",
    "    Y = np.full((X.shape[0], 1), class_label)  # Create an array of labels\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir0 = \"../data/Q2/train/dew\"\n",
    "train_dir1 = \"../data/Q2/train/fogsmog\"\n",
    "\n",
    "x0, y0 = preprocess_image(train_dir0, 0)\n",
    "x1, y1 = preprocess_image(train_dir1, 1)\n",
    "X_train = np.concatenate((x1, x0), axis=0)\n",
    "Y_train = np.concatenate((y1, y0), axis=0).ravel()\n",
    "\n",
    "test_dir0 = \"../data/Q2/test/dew\"\n",
    "test_dir1 = \"../data/Q2/test/fogsmog\"\n",
    "\n",
    "x2, y2 = preprocess_image(test_dir0, 0)\n",
    "x3, y3 = preprocess_image(test_dir1, 1)\n",
    "X_test = np.concatenate((x3, x2), axis=0)\n",
    "Y_test = np.concatenate((y3, y2), axis=0).ravel()\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svm import SupportVectorMachine\n",
    "import time\n",
    "# Initialize the SVM with a linear kernel\n",
    "svm_linear = SupportVectorMachine()\n",
    "\n",
    "# Train the SVM with C = 1.0\n",
    "start = time.time()\n",
    "svm_linear.fit(X_train, Y_train, kernel='linear', C=1.0)\n",
    "end = time.time()\n",
    "train_time_lin = end - start\n",
    "print(f\"Training time: {train_time_lin:.4f} seconds\")\n",
    "\n",
    "# Number of support vectors\n",
    "num_support_vectors = len(svm_linear.svalphas)\n",
    "percentage_support_vectors = (num_support_vectors / X_train.shape[0]) * 100\n",
    "\n",
    "print(f\"Number of support vectors: {num_support_vectors}\")\n",
    "print(f\"Percentage of training samples that are support vectors: {percentage_support_vectors:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "Y_pred_linear = svm_linear.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy_linear = np.mean(Y_pred_linear == Y_test) * 100\n",
    "print(f\"Test set accuracy (Linear Kernel): {test_accuracy_linear:.2f}%\")\n",
    "\n",
    "# Weight vector and intercept term\n",
    "print(f\"Weight vector (w): {svm_linear.w}\")\n",
    "print(f\"Intercept term (b): {svm_linear.b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get indices of top support vectors\n",
    "sorted_indices = np.argsort(svm_linear.svalphas)[::-1]  # Sort in descending order\n",
    "all_sv_images = svm_linear.svX[sorted_indices].reshape(-1, 100, 100, 3)\n",
    "\n",
    "# Filter unique support vectors\n",
    "unique_sv = []\n",
    "seen_hashes = set()\n",
    "\n",
    "for i, sv in enumerate(all_sv_images):\n",
    "    sv_flattened = tuple(sv.flatten())  # Convert to tuple for hashability\n",
    "    if sv_flattened not in seen_hashes:\n",
    "        seen_hashes.add(sv_flattened)\n",
    "        unique_sv.append((sorted_indices[i], sv))  # Keep index for reference\n",
    "    if len(unique_sv) == 5:  # Stop when we have 5 unique ones\n",
    "        break\n",
    "\n",
    "# Extract indices and images of unique support vectors\n",
    "top_5_indices, top_5_sv_images = zip(*unique_sv)\n",
    "\n",
    "# Compute weight vector image\n",
    "w_image = svm_linear.w.reshape(100, 100, 3)\n",
    "w_image = (w_image - w_image.min()) / (w_image.max() - w_image.min())  # Normalize\n",
    "\n",
    "# Plot support vectors and weight vector\n",
    "fig, axes = plt.subplots(1, 6, figsize=(15, 5))\n",
    "\n",
    "for i in range(len(top_5_sv_images)):\n",
    "    axes[i].imshow(top_5_sv_images[i])\n",
    "    axes[i].set_title(f\"Support Vector {i+1}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "# Plot the weight vector w as an image\n",
    "axes[5].imshow(w_image)\n",
    "axes[5].set_title(\"Weight Vector w\")\n",
    "axes[5].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svm import SupportVectorMachine\n",
    "# Initialize the SVM with a linear kernel\n",
    "svm_gaussian = SupportVectorMachine()\n",
    "\n",
    "# Train the SVM with C = 1.0\n",
    "start = time.time()\n",
    "svm_gaussian.fit(X_train, Y_train, kernel='gaussian', C=1.0, gamma=0.001)\n",
    "end = time.time()\n",
    "train_time_gaussian = end - start\n",
    "print(f\"Training time: {train_time_gaussian:.4f} seconds\")\n",
    "\n",
    "# Number of support vectors\n",
    "num_support_vectors = len(svm_gaussian.svalphas)\n",
    "percentage_support_vectors = (num_support_vectors / X_train.shape[0]) * 100\n",
    "\n",
    "print(f\"Number of support vectors: {num_support_vectors}\")\n",
    "print(f\"Percentage of training samples that are support vectors: {percentage_support_vectors:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "Y_pred_gaussian = svm_gaussian.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy_gaussian = np.mean(Y_pred_gaussian == Y_test) * 100\n",
    "print(f\"Test set accuracy (Gaussian Kernel): {test_accuracy_gaussian:.2f}%\")\n",
    "\n",
    "# Weight vector and intercept term\n",
    "print(f\"Intercept term (b): {svm_gaussian.b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_5_indices = np.argsort(svm_gaussian.svalphas)[-5:][::-1]\n",
    "top_5_sv_images = svm_gaussian.svX[top_5_indices].reshape(-1, 100, 100, 3)\n",
    "duplicates = np.sum(np.abs(top_5_sv_images[0] - top_5_sv_images[1]))\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].imshow(top_5_sv_images[i])\n",
    "    axes[i].set_title(f\"Support Vector {i+1}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract support vectors\n",
    "sv_linear = svm_linear.svX  # Support vectors from linear SVM\n",
    "sv_gaussian = svm_gaussian.svX  # Support vectors from Gaussian SVM\n",
    "\n",
    "# Check for common support vectors\n",
    "common_sv_count = sum(np.any(np.all(sv_linear[:, None] == sv_gaussian, axis=-1), axis=0))\n",
    "\n",
    "print(f\"Number of common support vectors: {common_sv_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the SVM with a linear kernel\n",
    "svm_sklearn_linear = SVC(kernel='linear', C=1.0)\n",
    "start = time.time()\n",
    "# Train the SVM with C = 1.0\n",
    "svm_sklearn_linear.fit(X_train, Y_train)\n",
    "end = time.time()\n",
    "\n",
    "# Predictions\n",
    "y_pred_lin = svm_sklearn_linear.predict(X_test)\n",
    "test_accuracy = accuracy_score(Y_test, y_pred_lin)\n",
    "\n",
    "print(\"SVMLinear Stats\")\n",
    "print(f\"Training time: {end - start:.2f}s\")\n",
    "print(f\"Number of Support Vectors: {len(svm_sklearn_linear.support_)}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "w_sklearn = np.dot(svm_sklearn_linear.dual_coef_, svm_sklearn_linear.support_vectors_).flatten()\n",
    "b_sklearn = svm_sklearn_linear.intercept_[0]\n",
    "print(f\"Sklearn SVM - w : {w_sklearn}\")\n",
    "print(f\"Sklearn SVM - w norm: {np.linalg.norm(w_sklearn)}\")\n",
    "print(f\"Sklearn SVM - bias (b): {b_sklearn}\")\n",
    "sv_sklearn = svm_sklearn_linear.support_vectors_  # Support vectors from Scikit-learn\n",
    "matching_sv_count_1 = sum(np.any(np.all(sv_linear[:, None] == sv_sklearn, axis=-1), axis=0))\n",
    "print(f\"Number of common support vectors (with CVXOPT linear): {matching_sv_count_1}\")\n",
    "matching_sv_count_2 = sum(np.any(np.all(sv_gaussian[:, None] == sv_sklearn, axis=-1), axis=0))\n",
    "print(f\"Number of common support vectors (with CVXOPT Gaussian): {matching_sv_count_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the SVM with a Gaussian (RBF) kernel\n",
    "svm_sklearn_rbf = SVC(kernel='rbf', C=1.0, gamma=0.001)\n",
    "start = time.time()\n",
    "# Train the SVM\n",
    "svm_sklearn_rbf.fit(X_train, Y_train)\n",
    "end = time.time()\n",
    "\n",
    "# Predictions\n",
    "y_pred_rbf = svm_sklearn_rbf.predict(X_test)\n",
    "test_accuracy_rbf = accuracy_score(Y_test, y_pred_rbf)\n",
    "\n",
    "print(\"\\nSVMGaussian Stats\")\n",
    "print(f\"Training time: {end - start:.2f}s\")\n",
    "print(f\"Number of Support Vectors: {len(svm_sklearn_rbf.support_)}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_rbf * 100:.2f}%\")\n",
    "\n",
    "# Extract support vectors\n",
    "sv_sklearn_rbf = svm_sklearn_rbf.support_vectors_\n",
    "\n",
    "# Compare with CVXOPT support vectors\n",
    "matching_sv_count_1 = sum(np.any(np.all(sv_linear[:, None] == sv_sklearn_rbf, axis=-1), axis=0))\n",
    "print(f\"Number of common support vectors (with CVXOPT linear): {matching_sv_count_1}\")\n",
    "matching_sv_count_2 = sum(np.any(np.all(sv_gaussian[:, None] == sv_sklearn_rbf, axis=-1), axis=0))\n",
    "print(f\"Number of common support vectors (with CVXOPT Gaussian): {matching_sv_count_2}\")\n",
    "\n",
    "# Bias term\n",
    "b_sklearn_rbf = svm_sklearn_rbf.intercept_[0]\n",
    "print(f\"Sklearn SVM - bias (b): {b_sklearn_rbf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the SGDClassifier with a hinge loss (which is equivalent to SVM)\n",
    "sgd_svm = SGDClassifier(loss='hinge', alpha=1e-4, max_iter=1000, tol=1e-4, random_state=42)\n",
    "\n",
    "# Train and measure time\n",
    "start = time.time()\n",
    "sgd_svm.fit(X_train, Y_train)\n",
    "end = time.time()\n",
    "\n",
    "# Predictions and accuracy\n",
    "y_pred_sgd = sgd_svm.predict(X_test)\n",
    "test_accuracy_sgd = accuracy_score(Y_test, y_pred_sgd)\n",
    "\n",
    "print(\"\\nSGD SVM Stats\")\n",
    "print(f\"Training time: {end - start:.2f}s\")\n",
    "print(f\"Number of Iterations: {sgd_svm.n_iter_}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_sgd * 100:.2f}%\")\n",
    "\n",
    "# Extract weight vector and bias\n",
    "w_sgd = sgd_svm.coef_.flatten()\n",
    "b_sgd = sgd_svm.intercept_[0]\n",
    "print(f\"SGD SVM - w norm: {np.linalg.norm(w_sgd)}\")\n",
    "print(f\"SGD SVM - bias (b): {b_sgd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ONE-VS-ONE MULTICLASS CLASSIFIER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(base_path):\n",
    "    classes = os.listdir(base_path)  # Maintain original order\n",
    "    class_map = {cls: i for i, cls in enumerate(classes)}  # Map class names to indices\n",
    "    class_dirs = {i: os.path.join(base_path, cls) for cls, i in class_map.items()}  # Use class name for path\n",
    "\n",
    "    # Count number of images in each class directory\n",
    "    # class_counts = {i: len(os.listdir(class_dirs[i])) for i in class_dirs}\n",
    "    # for i, count in class_counts.items():\n",
    "    #     print(f\"Class {i} ({list(class_map.keys())[i]}): {count} images\")\n",
    "    \n",
    "    return class_dirs, class_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dirs, train_map = load_data(\"../data/Q2/train\")\n",
    "test_dirs, test_map = load_data(\"../data/Q2/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "for i in range(11):\n",
    "    train_data[f\"x{i}_0\"], train_data[f\"y{i}_0\"] = preprocess_image(train_dirs[i], 0)\n",
    "    train_data[f\"x{i}_1\"], train_data[f\"y{i}_1\"] = preprocess_image(train_dirs[i], 1)\n",
    "\n",
    "# print(train_data.keys())\n",
    "\n",
    "# Print number of images per class variation\n",
    "# for i in range(11):\n",
    "#     print(f\"Class {i}: x{i}_0 - {len(train_data[f'x{i}_0'])} images, x{i}_1 - {len(train_data[f'x{i}_1'])} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svm import SupportVectorMachine\n",
    "from itertools import combinations\n",
    "svm_models = {}\n",
    "for i, j in combinations(range(11), 2):\n",
    "    X_train = np.concatenate((train_data[f\"x{i}_0\"], train_data[f\"x{j}_1\"]), axis=0)\n",
    "    Y_train = np.concatenate((train_data[f\"y{i}_0\"], train_data[f\"y{j}_1\"]), axis=0).ravel()\n",
    "    \n",
    "    model_name = f\"Bin_SVM_{i}_{j}\"\n",
    "    svm_models[(i,j)] = SupportVectorMachine()\n",
    "    print(f\"Training {model_name}...\")\n",
    "    svm_models[(i,j)].fit(X_train, Y_train, kernel='gaussian', C=1.0, gamma=0.001)\n",
    "    print(f\"Training complete for {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize empty arrays\n",
    "X_test, Y_test = None, None\n",
    "\n",
    "for i in range(11):  # Assuming 11 classes\n",
    "    X, Y = preprocess_image(test_dirs[i], i)  # Get processed images and labels\n",
    "\n",
    "    # Concatenate iteratively\n",
    "    if X_test is None:\n",
    "        X_test, Y_test = X, Y  # First batch initializes arrays\n",
    "    else:\n",
    "        X_test = np.concatenate((X_test, X), axis=0)\n",
    "        Y_test = np.concatenate((Y_test, Y), axis=0)\n",
    "\n",
    "# Flatten labels to 1D array\n",
    "Y_test = Y_test.ravel()\n",
    "\n",
    "# print(X_test.shape, Y_test.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_classes = 11  # Total number of classes\n",
    "num_samples = X_test.shape[0]  # Number of test images\n",
    "\n",
    "# Step 1: Initialize vote count and score sum arrays\n",
    "vote_counts = np.zeros((num_samples, num_classes), dtype=int)  # Shape: (num_samples, 11)\n",
    "score_sums = np.zeros((num_samples, num_classes), dtype=float)  # Shape: (num_samples, 11)\n",
    "\n",
    "# Step 2: Iterate through all trained SVM classifiers\n",
    "for (i, j), model in svm_models.items():\n",
    "    predictions, decision_values = model.predict_gaussian(X_test)  # Get decision values for all test samples\n",
    "\n",
    "    # Determine class assignments\n",
    "    pred_classes = np.where(predictions == 1, j, i)  # Assign class j if 1, else class i\n",
    "\n",
    "    # Update vote counts and score sums\n",
    "    for k in range(num_samples):\n",
    "        vote_counts[k, pred_classes[k]] += 1\n",
    "        score_sums[k, pred_classes[k]] += abs(decision_values[k])\n",
    "\n",
    "# Step 3: Determine the final predicted class for each test sample\n",
    "max_votes = np.max(vote_counts, axis=1, keepdims=True)  # Shape: (num_samples, 1)\n",
    "candidates = (vote_counts == max_votes)  # Boolean mask for classes with max votes\n",
    "\n",
    "# Resolve ties using score sums\n",
    "final_predictions = np.argmax(candidates * score_sums, axis=1)\n",
    "\n",
    "print(f\"Final predicted classes: {final_predictions}\")\n",
    "# Calculate test accuracy\n",
    "test_accuracy_multi = np.mean(final_predictions == Y_test) * 100\n",
    "print(f\"Test set accuracy: {test_accuracy_multi:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCI-KIT Implementation of multi-class SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize empty arrays\n",
    "X_train_sk, Y_train_sk = None, None\n",
    "\n",
    "for i in range(11):  # Assuming 11 classes\n",
    "    X, Y = preprocess_image(train_dirs[i], i)  # Get processed images and labels\n",
    "\n",
    "    # Concatenate iteratively\n",
    "    if X_train_sk is None:\n",
    "        X_train_sk, Y_train_sk = X, Y  # First batch initializes arrays\n",
    "    else:\n",
    "        X_train_sk = np.concatenate((X_train_sk, X), axis=0)\n",
    "        Y_train_sk = np.concatenate((Y_train_sk, Y), axis=0)\n",
    "\n",
    "# Flatten labels to 1D array\n",
    "Y_train_sk = Y_train_sk.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "svm_model_sk = SVC(kernel='rbf', C=1.0, gamma=0.001, decision_function_shape='ovo') \n",
    "start_time = time.time()\n",
    "svm_model_sk.fit(X_train_sk, Y_train_sk)\n",
    "training_time_sk = time.time() - start_time  # Measure training time\n",
    "print(f\"Training Time: {training_time_sk:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "Y_pred = svm_model_sk.predict(X_test)\n",
    "\n",
    "# Compute test accuracy\n",
    "test_accuracy_sk = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "print(f\"Test Set Accuracy: {test_accuracy_sk * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compute confusion matrices\n",
    "conf_matrix_cvx = confusion_matrix(Y_test, final_predictions)\n",
    "conf_matrix_svm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, class_map, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_map.keys(), yticklabels=class_map.keys())\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrix(conf_matrix_cvx, test_map, \"Confusion Matrix - CVXOPT\")\n",
    "plot_confusion_matrix(conf_matrix_svm, test_map, \"Confusion Matrix - LIBSVM (Scikit-Learn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify most frequently misclassified classes\n",
    "def analyze_misclassifications(conf_matrix, class_map):\n",
    "    misclassified = conf_matrix.copy()\n",
    "    np.fill_diagonal(misclassified, 0)  # Remove correct classifications\n",
    "    max_misclassified = np.unravel_index(np.argmax(misclassified), misclassified.shape)\n",
    "    \n",
    "    true_class = list(class_map.keys())[max_misclassified[0]]\n",
    "    predicted_class = list(class_map.keys())[max_misclassified[1]]\n",
    "    misclassified_count = misclassified[max_misclassified]\n",
    "\n",
    "    print(f\"Most misclassified class: {true_class} → {predicted_class} ({misclassified_count} times)\")\n",
    "    return max_misclassified\n",
    "\n",
    "# Analyze misclassifications\n",
    "print(\"CVXOPT Misclassification Analysis:\")\n",
    "cvx_misclassified = analyze_misclassifications(conf_matrix_cvx, test_map)\n",
    "\n",
    "print(\"\\nLIBSVM Misclassification Analysis:\")\n",
    "svm_misclassified = analyze_misclassifications(conf_matrix_svm, test_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 10 examples of misclassified objects\n",
    "def visualize_misclassified_examples(X_test, Y_test, Y_pred, true_class_idx, pred_class_idx, class_map, num_samples=10):\n",
    "    misclassified_indices = np.where((Y_test == true_class_idx) & (Y_pred == pred_class_idx))[0]\n",
    "\n",
    "    if len(misclassified_indices) > num_samples:\n",
    "        misclassified_indices = np.random.choice(misclassified_indices, num_samples, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(misclassified_indices), figsize=(15, 5))\n",
    "    for i, idx in enumerate(misclassified_indices):\n",
    "        axes[i].imshow(X_test[idx].reshape(28, 28), cmap=\"gray\")  # Adjust reshape based on dataset\n",
    "        axes[i].set_title(f\"True: {class_map[Y_test[idx]]}\\nPred: {class_map[Y_pred[idx]]}\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visualize misclassified examples for CVXOPT\n",
    "print(\"Misclassified examples from CVXOPT:\")\n",
    "visualize_misclassified_examples(X_test, Y_test, final_predictions, cvx_misclassified[0], cvx_misclassified[1], test_map)\n",
    "\n",
    "# Visualize misclassified examples for LIBSVM\n",
    "print(\"Misclassified examples from LIBSVM:\")\n",
    "visualize_misclassified_examples(X_test, Y_test, Y_pred, svm_misclassified[0], svm_misclassified[1], test_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Assuming X_train_sk, Y_train_sk, X_test, Y_test are already defined\n",
    "\n",
    "# Shuffle the training data to ensure randomness\n",
    "X_train, Y_train = shuffle(X_train_sk, Y_train_sk, random_state=42)\n",
    "\n",
    "# Define values of C to test\n",
    "C_values = [1e-5, 1e-3, 1, 5, 10]\n",
    "cv_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for C in C_values:\n",
    "    svm = SVC(C=C, gamma=0.001, kernel='rbf', decision_function_shape='ovo')  # One-vs-One strategy\n",
    "    \n",
    "    # Compute cross-validation accuracy\n",
    "    scores = cross_val_score(svm, X_train, Y_train, cv=kf, scoring='accuracy')\n",
    "    mean_cv_score = scores.mean()\n",
    "    cv_scores.append(mean_cv_score)\n",
    "    \n",
    "    # Train model on entire training set\n",
    "    svm.fit(X_train, Y_train)\n",
    "    \n",
    "    # Compute test accuracy\n",
    "    test_pred = svm.predict(X_test)\n",
    "    test_accuracy = accuracy_score(Y_test, test_pred)\n",
    "    test_scores.append(test_accuracy)\n",
    "    \n",
    "    print(f'C={C}: CV Accuracy={mean_cv_score:.4f}, Test Accuracy={test_accuracy:.4f}')\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(C_values, cv_scores, marker='o', label='5-Fold CV Accuracy')\n",
    "plt.plot(C_values, test_scores, marker='s', label='Test Accuracy')\n",
    "plt.xscale('log')  # Log scale for better visualization\n",
    "plt.xlabel('C (log scale)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-validation & Test Accuracy vs C')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the best C based on CV accuracy\n",
    "best_C = C_values[np.argmax(cv_scores)]\n",
    "print(f'Best C from CV: {best_C}')\n",
    "\n",
    "# Train final model using best C\n",
    "final_svm = SVC(C=best_C, gamma=0.001, kernel='rbf', decision_function_shape='ovo')  # One-vs-One strategy\n",
    "final_svm.fit(X_train, Y_train)\n",
    "final_test_pred = final_svm.predict(X_test)\n",
    "final_test_accuracy = accuracy_score(Y_test, final_test_pred)\n",
    "print(f'Final model test accuracy with C={best_C}: {final_test_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_a1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
